{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# import csv\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # Set up OpenAI API\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_base = os.getenv(\"ENDPOINT\")\n",
    "# openai.api_version = \"2024-02-15-preview\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # Create a client\n",
    "# client = openai.AzureOpenAI(\n",
    "#     api_key=openai.api_key,\n",
    "#     api_version=openai.api_version,\n",
    "#     azure_endpoint=openai.api_base,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Function to read file content\n",
    "# def read_file(file_path):\n",
    "#     with open(file_path, \"r\") as file:\n",
    "#         return file.read()\n",
    "\n",
    "\n",
    "# # Read input files\n",
    "# soa_mapping = read_file(\"outputs/output_activity_to_forms_v2.txt\")\n",
    "# visit_schedule = read_file(\"outputs/output_study_timeline.txt\")\n",
    "# visit_extractions = read_file(\"outputs/output_visit_extraction_v1.txt\")\n",
    "# cdash_info = read_file(\"CDASH-2-3-IG.csv\")\n",
    "\n",
    "# # Construct the prompt\n",
    "# prompt = f\"\"\"\n",
    "# You are a clinical data management expert tasked with creating a comprehensive Data Management Plan for a clinical trial. You have been provided with four key inputs:\n",
    "\n",
    "# 1. A table mapping activities from the Schedule of Activities (SOA) to appropriate eCRF forms:\n",
    "# {soa_mapping}\n",
    "\n",
    "# 2. A visit schedule extracted from the SOA file:\n",
    "# {visit_schedule}\n",
    "\n",
    "# 3. A table of visit extractions including visit names, descriptions, labels, windows, and types:\n",
    "# {visit_extractions}\n",
    "\n",
    "# 4. Clinical Data Acquisition Standards Harmonization (CDASH) information for various domains:\n",
    "# {cdash_info}\n",
    "\n",
    "# Using these inputs, create a Data Management Plan table with the following columns:\n",
    "# 1. Check ID (use format DRP-XXXXX)\n",
    "# 2. Data Page / Data Grouping\n",
    "# 3. Data Source (typically EDC or Vendor)\n",
    "# 4. Critical Data Yes/No\n",
    "# 5. Data Review Type\n",
    "# 6. Data Review Role\n",
    "# 7. Data Review Frequency\n",
    "# 8. Applicable Visits\n",
    "# 9. Check Description\n",
    "# 10. Query Text\n",
    "# 11. Review Comments (leave blank for now)\n",
    "\n",
    "# For each row in the table:\n",
    "# - Choose a relevant activity from the SOA mapping.\n",
    "# - Determine if it's critical data based on its nature and importance in the study.\n",
    "# - Assign an appropriate data review type (e.g., Eligibility, Missing Data, Data Quality, Protocol Compliance, Safety).\n",
    "# - Assign a suitable review role (e.g., Data Manager, Medical Monitor, Clinical Monitor) based on the activity.\n",
    "# - Set a review frequency appropriate for the data type and criticality.\n",
    "# - Specify applicable visits based on the visit schedule and extractions.\n",
    "# - Write a clear check description related to the data point.\n",
    "# - Create a specific query text that would be used if the check fails.\n",
    "\n",
    "# Ensure that your Data Management Plan covers a range of important study activities, includes both critical and non-critical data, and addresses various aspects of data review and quality control. The plan should be tailored to the specific study based on the provided inputs.\n",
    "\n",
    "# Please generate a Data Management Plan table with at least 10 rows, formatted as a CSV for easy reading and further processing.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the task into multiple API calls, since the data is too big to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# import csv\n",
    "# from dotenv import load_dotenv\n",
    "# import re\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # Set up OpenAI API\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_base = os.getenv(\"ENDPOINT\")\n",
    "# openai.api_version = \"2024-02-15-preview\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # Create a client\n",
    "# client = openai.AzureOpenAI(\n",
    "#     api_key=openai.api_key,\n",
    "#     api_version=openai.api_version,\n",
    "#     azure_endpoint=openai.api_base,\n",
    "# )\n",
    "\n",
    "\n",
    "# def clean_output(text):\n",
    "#     # Remove any lines that start with \"Certainly!\" or similar phrases\n",
    "#     lines = text.split(\"\\n\")\n",
    "#     cleaned_lines = [\n",
    "#         line\n",
    "#         for line in lines\n",
    "#         if not re.match(r\"^(Certainly!|Here are|Sure,)\", line.strip())\n",
    "#     ]\n",
    "\n",
    "#     # Remove any content enclosed in triple backticks\n",
    "#     cleaned_text = re.sub(r\"```[\\s\\S]*?```\", \"\", \"\\n\".join(cleaned_lines))\n",
    "\n",
    "#     # Remove any remaining empty lines\n",
    "#     cleaned_text = \"\\n\".join(line for line in cleaned_text.split(\"\\n\") if line.strip())\n",
    "\n",
    "#     return cleaned_text\n",
    "\n",
    "\n",
    "# # Function to read file content\n",
    "# def read_file(file_path):\n",
    "#     with open(file_path, \"r\") as file:\n",
    "#         return file.read()\n",
    "\n",
    "\n",
    "# # Read input files\n",
    "# soa_mapping = read_file(\"outputs/output_activity_to_forms_v2.txt\")\n",
    "# visit_schedule = read_file(\"outputs/output_study_timeline.txt\")\n",
    "# visit_extractions = read_file(\"outputs/output_visit_extraction_v1.txt\")\n",
    "# cdash_info = read_file(\"CDASH-2-3-IG.csv\")\n",
    "\n",
    "\n",
    "# # Function to call OpenAI API\n",
    "# def call_openai_api(prompt):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # Make sure this is the correct model name for your Azure deployment\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a clinical data management expert.\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt},\n",
    "#         ],\n",
    "#         temperature=0.7,\n",
    "#         max_tokens=2000,\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# # Function to chunk large text\n",
    "# def chunk_text(text, chunk_size=500000):\n",
    "#     return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Management Plan has been generated and saved as 'data_management_plan.csv'.\n"
     ]
    }
   ],
   "source": [
    "# def generate_data_management_plan(\n",
    "#     soa_mapping, visit_schedule, visit_extractions, cdash_info\n",
    "# ):\n",
    "#     header = \"Check ID\\tData Page / Data Grouping\\tData Source\\tCritical Data Yes/No\\tData Review Type\\tData Review Role\\tData Review Frequency\\tApplicable Visits\\tCheck Description\\tQuery Text\\tReview Comments\"\n",
    "\n",
    "#     initial_prompt = f\"\"\"Create a Data Management Plan table for a clinical trial. Use the following header:\n",
    "\n",
    "# {header}\n",
    "\n",
    "# Generate 5 rows for the Data Management Plan. Each row should follow this format and be separated by tabs. Ensure that:\n",
    "# 1. Check IDs follow the format DRP-XXXXX (where XXXXX is a 5-digit number).\n",
    "# 2. Data Source is typically EDC or Vendor.\n",
    "# 3. Critical Data is either Yes or No.\n",
    "# 4. Data Review Role can be Data Manager, Clinical Monitor, Medical Monitor, etc.\n",
    "# 5. Data Review Frequency can be Daily, Weekly, Bi-weekly, Monthly, etc.\n",
    "# 6. Applicable Visits should be specific (e.g., SCR 1, Cycle 1 Day 1, EOT, etc.).\n",
    "# 7. The Review Comments column should be left empty.\n",
    "\n",
    "# Ensure diversity in data groupings, review types, and visit applicability.\"\"\"\n",
    "\n",
    "#     table_rows = call_openai_api(initial_prompt)\n",
    "#     full_table = header + \"\\n\" + table_rows\n",
    "\n",
    "#     # Process SOA mapping\n",
    "#     soa_prompt = f\"\"\"Based on this SOA mapping:\n",
    "\n",
    "# {soa_mapping}\n",
    "\n",
    "# Add 5 more rows to the Data Management Plan table. Follow the same format as before, ensuring diversity in data groupings, review types, and visit applicability. Provide only the new rows, separated by tabs.\"\"\"\n",
    "\n",
    "#     soa_rows = call_openai_api(soa_prompt)\n",
    "#     full_table += \"\\n\" + soa_rows\n",
    "\n",
    "#     # Process visit schedule\n",
    "#     visit_prompt = f\"\"\"Based on this visit schedule:\n",
    "\n",
    "# {visit_schedule}\n",
    "\n",
    "# Add 5 more rows to the Data Management Plan table, focusing on visit-specific checks. Follow the same format as before. Provide only the new rows, separated by tabs.\"\"\"\n",
    "\n",
    "#     visit_rows = call_openai_api(visit_prompt)\n",
    "#     full_table += \"\\n\" + visit_rows\n",
    "\n",
    "#     # Process visit extractions\n",
    "#     extraction_prompt = f\"\"\"Based on these visit extractions:\n",
    "\n",
    "# {visit_extractions}\n",
    "\n",
    "# Add 5 more rows to the Data Management Plan table, focusing on data quality and completeness checks. Follow the same format as before. Provide only the new rows, separated by tabs.\"\"\"\n",
    "\n",
    "#     extraction_rows = call_openai_api(extraction_prompt)\n",
    "#     full_table += \"\\n\" + extraction_rows\n",
    "\n",
    "#     # Process CDASH info\n",
    "#     cdash_prompt = f\"\"\"Based on this CDASH information:\n",
    "\n",
    "# {cdash_info[:1000]}  # Using only the first 1000 characters to avoid token limit\n",
    "\n",
    "# Add 5 more rows to the Data Management Plan table, focusing on CDASH-specific data checks. Follow the same format as before. Provide only the new rows, separated by tabs.\"\"\"\n",
    "\n",
    "#     cdash_rows = call_openai_api(cdash_prompt)\n",
    "#     full_table += \"\\n\" + cdash_rows\n",
    "\n",
    "#     return full_table\n",
    "\n",
    "\n",
    "# # Generate the Data Management Plan\n",
    "# generated_table = generate_data_management_plan(\n",
    "#     soa_mapping, visit_schedule, visit_extractions, cdash_info\n",
    "# )\n",
    "\n",
    "# # Save the generated table as a CSV file\n",
    "# with open(\"data_management_plan.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "#     file.write(generated_table)\n",
    "\n",
    "# print(\n",
    "#     \"Data Management Plan has been generated and saved as 'data_management_plan.csv'.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter by domain_name and field_core(hr and r/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Management Plan has been generated and saved as 'data_management_plan.csv'.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# from dotenv import load_dotenv\n",
    "# import re\n",
    "# import csv\n",
    "# from io import StringIO\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # Set up OpenAI API\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_base = os.getenv(\"ENDPOINT\")\n",
    "# openai.api_version = \"2024-02-15-preview\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # Create a client\n",
    "# client = openai.AzureOpenAI(\n",
    "#     api_key=openai.api_key,\n",
    "#     api_version=openai.api_version,\n",
    "#     azure_endpoint=openai.api_base\n",
    "# )\n",
    "\n",
    "# # Function to read file content\n",
    "# def read_file(file_path):\n",
    "#     with open(file_path, \"r\") as file:\n",
    "#         return file.read()\n",
    "\n",
    "# # Read input files\n",
    "# soa_mapping = read_file(\"outputs/output_activity_to_forms_v2.txt\")\n",
    "# visit_schedule = read_file(\"outputs/output_study_timeline.txt\")\n",
    "# visit_extractions = read_file(\"outputs/output_visit_extraction_v1.txt\")\n",
    "# cdash_info = read_file(\"CDASH-2-3-IG.csv\")\n",
    "\n",
    "# def call_openai_api(prompt):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # Make sure this is the correct model name for your Azure deployment\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a clinical data management expert. Provide only the requested table rows without any additional text or formatting.\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt},\n",
    "#         ],\n",
    "#         temperature=0.7,\n",
    "#         max_tokens=2000,\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# def clean_output(text):\n",
    "#     # Remove any lines that don't look like our expected data\n",
    "#     lines = text.split('\\n')\n",
    "#     cleaned_lines = [line for line in lines if re.match(r'^DRP-\\d{5}\\t', line)]\n",
    "#     return '\\n'.join(cleaned_lines)\n",
    "\n",
    "# def filter_and_chunk_cdash_by_domain(cdash_info):\n",
    "#     # Parse the CSV data\n",
    "#     csv_data = csv.reader(StringIO(cdash_info))\n",
    "#     header = next(csv_data)\n",
    "    \n",
    "#     # Find the indices of the relevant columns\n",
    "#     domain_index = header.index('domain_name')\n",
    "#     core_index = header.index('field_core')\n",
    "    \n",
    "#     # Group rows by domain_name, filtering for HR and R/C fields\n",
    "#     domains = {}\n",
    "#     for row in csv_data:\n",
    "#         if row[core_index] in ['HR', 'R/C']:\n",
    "#             domain = row[domain_index]\n",
    "#             if domain not in domains:\n",
    "#                 domains[domain] = [header]\n",
    "#             domains[domain].append(row)\n",
    "    \n",
    "#     # Convert each domain group back to CSV string\n",
    "#     return {domain: '\\n'.join([','.join(row) for row in rows]) for domain, rows in domains.items()}\n",
    "\n",
    "# def generate_data_management_plan(soa_mapping, visit_schedule, visit_extractions, cdash_info):\n",
    "#     header = \"Check ID\\tData Page / Data Grouping\\tData Source\\tCritical Data Yes/No\\tData Review Type\\tData Review Role\\tData Review Frequency\\tApplicable Visits\\tCheck Description\\tQuery Text\\tReview Comments\"\n",
    "    \n",
    "#     full_table = header + \"\\n\"\n",
    "\n",
    "#     # Process SOA mapping\n",
    "#     soa_prompt = f\"\"\"Based on this SOA mapping:\n",
    "\n",
    "# {soa_mapping}\n",
    "\n",
    "# Create rows for a Data Management Plan table. Each row should follow this format and be separated by tabs:\n",
    "\n",
    "# {header}\n",
    "\n",
    "# Ensure that:\n",
    "# 1. Check IDs follow the format DRP-XXXXX (where XXXXX is a 5-digit number).\n",
    "# 2. Data Source is typically EDC or Vendor.\n",
    "# 3. Critical Data is either Yes or No.\n",
    "# 4. Data Review Role can be Data Manager, Clinical Monitor, Medical Monitor, etc.\n",
    "# 5. Data Review Frequency can be Daily, Weekly, Bi-weekly, Monthly, etc.\n",
    "# 6. Applicable Visits should be specific (e.g., SCR 1, Cycle 1 Day 1, EOT, etc.).\n",
    "# 7. The Review Comments column should be left empty.\n",
    "\n",
    "# Provide only the data rows, without the header or any additional text.\"\"\"\n",
    "\n",
    "#     soa_rows = call_openai_api(soa_prompt)\n",
    "#     full_table += clean_output(soa_rows) + \"\\n\"\n",
    "\n",
    "#     # Process visit schedule\n",
    "#     visit_prompt = f\"\"\"Based on this visit schedule:\n",
    "\n",
    "# {visit_schedule}\n",
    "\n",
    "# Add more rows to the Data Management Plan table, focusing on visit-specific checks. Follow the same format as before. Provide only the new rows, without any additional text.\"\"\"\n",
    "\n",
    "#     visit_rows = call_openai_api(visit_prompt)\n",
    "#     full_table += clean_output(visit_rows) + \"\\n\"\n",
    "\n",
    "#     # Process visit extractions\n",
    "#     extraction_prompt = f\"\"\"Based on these visit extractions:\n",
    "\n",
    "# {visit_extractions}\n",
    "\n",
    "# Add more rows to the Data Management Plan table, focusing on data quality and completeness checks. Follow the same format as before. Provide only the new rows, without any additional text.\"\"\"\n",
    "\n",
    "#     extraction_rows = call_openai_api(extraction_prompt)\n",
    "#     full_table += clean_output(extraction_rows) + \"\\n\"\n",
    "\n",
    "#     # Process CDASH info by domain\n",
    "#     cdash_chunks = filter_and_chunk_cdash_by_domain(cdash_info)\n",
    "#     for domain, chunk in cdash_chunks.items():\n",
    "#         cdash_prompt = f\"\"\"Based on the CDASH information for the {domain} domain:\n",
    "\n",
    "# {chunk}\n",
    "\n",
    "# Add rows to the Data Management Plan table, focusing on CDASH-specific data checks for this domain. Consider only the Highly Recommended (HR) and Required/Conditional (R/C) fields. Follow the same format as before. Provide only the new rows, without any additional text.\"\"\"\n",
    "\n",
    "#         cdash_rows = call_openai_api(cdash_prompt)\n",
    "#         full_table += clean_output(cdash_rows) + \"\\n\"\n",
    "\n",
    "#     return full_table\n",
    "\n",
    "# # Generate the Data Management Plan\n",
    "# generated_table = generate_data_management_plan(soa_mapping, visit_schedule, visit_extractions, cdash_info)\n",
    "\n",
    "# # Save the generated table as a CSV file\n",
    "# with open(\"data_management_plan.csv\", \"w\", newline=\"\", encoding='utf-8') as file:\n",
    "#     file.write(generated_table)\n",
    "\n",
    "# print(\"Data Management Plan has been generated and saved as 'data_management_plan.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing time is too long, add those method:\n",
    "- Parallel processing: use `concurrent.futures.ThreadPoolExecutor` to make API calls concurrently.\n",
    "- Caching: add `@lru_cache` to the `call_openai_api` function to cache responses and avoid redundant API calls.\n",
    "- Batching: batch CDASH domains together (3 domains per batch) to reduce the number of API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Management Plan has been generated and saved as 'data_management_plan.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import csv\n",
    "from io import StringIO\n",
    "import concurrent.futures\n",
    "import json\n",
    "from functools import lru_cache\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"ENDPOINT\")\n",
    "openai.api_version = \"2024-02-15-preview\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Create a client\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=openai.api_key,\n",
    "    api_version=openai.api_version,\n",
    "    azure_endpoint=openai.api_base,\n",
    ")\n",
    "\n",
    "\n",
    "# Function to read file content\n",
    "def read_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "# Read input files\n",
    "soa_mapping = read_file(\"outputs/output_activity_to_forms_v2.txt\")\n",
    "visit_schedule = read_file(\"outputs/output_study_timeline.txt\")\n",
    "visit_extractions = read_file(\"outputs/output_visit_extraction_v1.txt\")\n",
    "cdash_info = read_file(\"CDASH-2-3-IG.csv\")\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def call_openai_api(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  \n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a clinical data management expert. Provide only the requested table rows without any additional text or formatting.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def clean_output(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = [line for line in lines if re.match(r\"^DRP-\\d{5}\\t\", line)]\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "\n",
    "def filter_and_chunk_cdash_by_domain(cdash_info):\n",
    "    csv_data = csv.reader(StringIO(cdash_info))\n",
    "    header = next(csv_data)\n",
    "\n",
    "    domain_index = header.index(\"domain_name\")\n",
    "    core_index = header.index(\"field_core\")\n",
    "    name_index = header.index(\"field_name\")\n",
    "    label_index = header.index(\"field_label\")\n",
    "\n",
    "    domains = {}\n",
    "    for row in csv_data:\n",
    "        if row[core_index] in [\"HR\", \"R/C\"]:\n",
    "            domain = row[domain_index]\n",
    "            if domain not in domains:\n",
    "                domains[domain] = []\n",
    "            domains[domain].append(f\"{row[name_index]}: {row[label_index]}\")\n",
    "\n",
    "    return domains\n",
    "\n",
    "\n",
    "def process_chunk(chunk_type, chunk_data):\n",
    "    header = \"Check ID\\tData Page / Data Grouping\\tData Source\\tCritical Data Yes/No\\tData Review Type\\tData Review Role\\tData Review Frequency\\tApplicable Visits\\tCheck Description\\tQuery Text\\tReview Comments\"\n",
    "\n",
    "    if chunk_type == \"soa\":\n",
    "        prompt = f\"\"\"Based on this SOA mapping:\n",
    "\n",
    "{chunk_data}\n",
    "\n",
    "Create rows for a Data Management Plan table. Each row should follow this format and be separated by tabs:\n",
    "\n",
    "{header}\n",
    "\n",
    "Ensure that:\n",
    "1. Check IDs follow the format DRP-XXXXX (where XXXXX is a 5-digit number).\n",
    "2. Data Source is typically EDC or Vendor.\n",
    "3. Critical Data is either Yes or No.\n",
    "4. Data Review Role can be Data Manager, Clinical Monitor, Medical Monitor, etc.\n",
    "5. Data Review Frequency can be Daily, Weekly, Bi-weekly, Monthly, etc.\n",
    "6. Applicable Visits should be specific (e.g., SCR 1, Cycle 1 Day 1, EOT, etc.).\n",
    "7. The Review Comments column should be left empty.\n",
    "\n",
    "Provide only the data rows, without the header or any additional text.\"\"\"\n",
    "    elif chunk_type == \"visit\":\n",
    "        prompt = f\"\"\"Based on this visit information:\n",
    "\n",
    "{chunk_data}\n",
    "\n",
    "Add more rows to the Data Management Plan table, focusing on visit-specific checks. Follow the same format as before. Provide only the new rows, without any additional text.\"\"\"\n",
    "    elif chunk_type == \"cdash\":\n",
    "        prompt = f\"\"\"Based on the CDASH information for the following domains:\n",
    "\n",
    "{json.dumps(chunk_data, indent=2)}\n",
    "\n",
    "Add rows to the Data Management Plan table, focusing on CDASH-specific data checks for these domains. Consider only the Highly Recommended (HR) and Required/Conditional (R/C) fields provided. Follow the same format as before. Provide only the new rows, without any additional text.\"\"\"\n",
    "\n",
    "    rows = call_openai_api(prompt)\n",
    "    return clean_output(rows)\n",
    "\n",
    "\n",
    "def generate_data_management_plan(\n",
    "    soa_mapping, visit_schedule, visit_extractions, cdash_info\n",
    "):\n",
    "    header = \"Check ID\\tData Page / Data Grouping\\tData Source\\tCritical Data Yes/No\\tData Review Type\\tData Review Role\\tData Review Frequency\\tApplicable Visits\\tCheck Description\\tQuery Text\\tReview Comments\"\n",
    "\n",
    "    cdash_chunks = filter_and_chunk_cdash_by_domain(cdash_info)\n",
    "\n",
    "    # Batch CDASH domains\n",
    "    batched_cdash = {}\n",
    "    batch_size = 3\n",
    "    for i in range(0, len(cdash_chunks), batch_size):\n",
    "        batch = list(cdash_chunks.items())[i : i + batch_size]\n",
    "        batched_cdash[f\"batch_{i//batch_size}\"] = dict(batch)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        futures.append(executor.submit(process_chunk, \"soa\", soa_mapping))\n",
    "        futures.append(executor.submit(process_chunk, \"visit\", visit_schedule))\n",
    "        futures.append(executor.submit(process_chunk, \"visit\", visit_extractions))\n",
    "        for batch in batched_cdash.values():\n",
    "            futures.append(executor.submit(process_chunk, \"cdash\", batch))\n",
    "\n",
    "        results = [\n",
    "            future.result() for future in concurrent.futures.as_completed(futures)\n",
    "        ]\n",
    "\n",
    "    full_table = header + \"\\n\" + \"\\n\".join(results)\n",
    "    return full_table\n",
    "\n",
    "\n",
    "# Generate the Data Management Plan\n",
    "generated_table = generate_data_management_plan(\n",
    "    soa_mapping, visit_schedule, visit_extractions, cdash_info\n",
    ")\n",
    "\n",
    "# Save the generated table as a CSV file\n",
    "with open(\"data_management_plan.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    file.write(generated_table)\n",
    "\n",
    "print(\n",
    "    \"Data Management Plan has been generated and saved as 'data_management_plan.csv'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
